{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b11dc80-c1b0-45ce-8c19-4d0a348a10df",
   "metadata": {},
   "source": [
    "# Importing and saving WINDS dataset\n",
    "\n",
    "## Example for: April 1997 - half-hour dataset for 70 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3feec8-5b45-4f5c-ba78-c60acd8a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f1ac4-0617-4585-8d16-095afecd4173",
   "metadata": {},
   "source": [
    "### Loading the half-hour velocity dataset using xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600a10c-85be-49cf-a0a1-a741472fc419",
   "metadata": {},
   "source": [
    "Some problems with the data access may occur. It's better to save the dataset to have it always available, even it is quite heavy. After clipping the unnecessary variables the dataset for 301x301 grid points has a size of 1.7GB, for 100 days of half-hour observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff37bab6-4322-4a6c-99a0-26b28dc00a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_1997 = 'https://dap.ceda.ac.uk/thredds/dodsC/bodc/UOX220077/WINDS-M/1997/WINDS-M_SFC_1997.nc'\n",
    "ds_complete_1997 = xr.open_dataset(base_url_1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94450f4-21fa-4f40-8cc3-668f92d83e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary variables\n",
    "dropping_variables_1997 = ds_complete_1997.drop_vars(['s_w','hc','s_rho',\n",
    "                                 'theta_s','theta_b','Tcline','Vtransform','h','f','pm','pn','lon_rho','lat_rho',\n",
    "                                  'angle','mask_rho', 'Cs_r','sc_r','Cs_w','lon_u','lon_v','lat_u', 'lat_v',\n",
    "                                  'sc_w'])\n",
    "\n",
    "# Clipping considering only the domain of interest for the thesis case study\n",
    "clipped_domain_1997 = dropping_variables_1997.sel(y_rho=slice(800, 1101), x_rho=slice(150, 451),\n",
    "                          y_u=slice(800, 1101), x_u=slice(150, 451),\n",
    "                          y_v=slice(800, 1101), x_v=slice(150, 451))\n",
    "\n",
    "# Considering only 70 + 30 days from 1st April (so 100*24*2+1) values of 'APRIL', so from 01-04-1997 to 09-07-1997, so from 4320 to 4320+4801\n",
    "days100_from_april_1997 = clipped_domain_1997.isel(time_counter=slice(4320, 4320+4801))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d2a03-c1e5-41eb-8030-8bbe780859b3",
   "metadata": {},
   "source": [
    "- The complete dataset has these dimensions (rho_points from 34.62 to 77.50 and from -23.50 to 0.01111261):\n",
    "\n",
    "time_counter: 17520, y_rho: 1211, x_rho: 2145, s_rho: 50, s_w: 51, y_v: 1210, x_v: 214, 5, y_u: 1211, x_u: 2144\n",
    "\n",
    "\n",
    "- For the thesis case study dataset (rho_points from 37.62 to 43.62 and from -8.161049 to -2.1883478):\n",
    "\n",
    "time_counter: 4801, y_rho: 301, x_rho: 301, y_v: 301, x_v: 301"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72324c8e-858a-490a-acf3-b115c3fe99df",
   "metadata": {},
   "source": [
    "##### NOTE: Case of November as spawning month\n",
    "\n",
    "Since starting from 1st November 1997, the 100th day after is the 8th February 1998, the 1998 dataset must be used too, with the same process done above.\n",
    "\n",
    "To concatenate the two dataset with 61 + 39 days, it is possible to proceed in this way (it is very fast [9sec with rechunking, 15min without]):\n",
    "- Chunk sizes\n",
    "  \n",
    "chunk_sizes = {'time_counter': 1, 'y_rho': 301, 'x_rho': 301, 'y_v': 301, 'x_v': 301, 'y_u': 301, 'x_u': 301}\n",
    "- Rechunking the two datasets (the one with values of November and December and the one of January and February)\n",
    "  \n",
    "nov_dec_1997_rechunked = nov_dec_1997.chunk(chunk_sizes)\n",
    "\n",
    "jan_feb_1998_rechunked = jan_feb_1998.chunk(chunk_sizes)\n",
    "- Creating the rechunked 100 days dataset\n",
    "  \n",
    "days70_from_november_1997 = xr.concat([nov_dec_1997_rechunked, jan_feb_1998_rechunked], dim='time_counter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf25615-1f9c-4e55-a375-e8910dddab61",
   "metadata": {},
   "source": [
    "### Mesh grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154e4d6-4cc7-4b9a-9484-c73390e8bd99",
   "metadata": {},
   "source": [
    "The same process of velocities. NOTE that the mesh grid is unique, so it has not to be imported for every period of study (ex. November and April), but just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d8f9b-ad5f-4d83-8f88-0a9b2a424da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_url = 'https://dap.ceda.ac.uk/thredds/dodsC/bodc/UOX220077/WINDS-M/supplementary_files/croco_grd.nc'\n",
    "ds_mask = xr.open_dataset(mask_url)\n",
    "\n",
    "clipped_mask_one = ds_mask.sel(eta_rho=slice(800, 1101), xi_rho=slice(150, 451),\n",
    "                               eta_psi=slice(800, 1101), xi_psi=slice(150, 451),\n",
    "                               eta_u=slice(800, 1101), xi_u=slice(150, 451),\n",
    "                               eta_v=slice(800, 1101), xi_v=slice(150, 451)) # clipping the domain\n",
    "\n",
    "clipped_mask = clipped_mask_one.drop_vars(['xl','el','depthmin','depthmax', 'spherical', 'angle', 'h', 'hraw', 'alpha',\n",
    "                                 'f','pm','pn','dndx','dmde','x_rho','x_u','x_v','x_psi','y_rho',\n",
    "                                  'y_v','y_u', 'y_psi', 'mask_u', 'mask_v', 'mask_psi'   \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9055e7-768f-46d7-add3-e7318b2b147e",
   "metadata": {},
   "source": [
    "### Velocity fields and mesh grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f04eb4-379a-488c-b3b8-cb108b68245a",
   "metadata": {},
   "source": [
    "To better handle with OceanParcels is a good practice to split the u surface velocity field (u_surf) and the v surface velocity field (v_surf). It is also necessary to have in the FieldSet the mesh grid, so it must be imported too, as done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7d776-5961-4e8f-835d-bb0304dbb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "days100_from_april_1997_velocity_u = days100_from_april_1997.drop_vars(['nav_lon_v', 'nav_lat_v', 'v_surf', 'time'])\n",
    "days100_from_april_1997_velocity_v = days100_from_april_1997.drop_vars(['nav_lon_u', 'nav_lat_u', 'u_surf', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed21850-dca0-4c04-8db4-64499611d94b",
   "metadata": {},
   "source": [
    "### Saving datasets into nc files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4271e1f-a387-4654-9c85-4b7b1e9f1c94",
   "metadata": {},
   "source": [
    "To save the dataset locally this method works (U velocity field for 1st April 1997 - 9th July 1997):\n",
    "\n",
    "chunks = {'time_counter': 100, 'y_u': 301, 'x_u': 301}\n",
    "\n",
    "days100_from_april_1997_velocity_u_chunked = days100_from_april_1997_velocity_u.chunk(chunks)\n",
    "\n",
    "days100_from_april_1997_velocity_u_chunked.to_netcdf(\"days100_from_april_1997_velocity_u_OK.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8a68f-3ce9-4b66-89bb-8df4fd5f0854",
   "metadata": {},
   "source": [
    "### Opening saved datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1454cf5-2aca-473a-9fae-e27a254ac4b8",
   "metadata": {},
   "source": [
    "It is very easy and quick: just naming a variable for the dataset and use 'xr.open.dataset' function\n",
    "\n",
    "days100_from_april_1997_velocity_u_OK = xr.open_dataset(\"days70_from_april_1997_velocity_u_OK.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
